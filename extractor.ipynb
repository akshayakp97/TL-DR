{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class for StanfordNER Tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "#refer links - https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/\n",
    "#refer links - https://blog.sicara.com/train-ner-model-with-nltk-stanford-tagger-english-french-german-6d90573a9486\n",
    "\n",
    "class MyStanfordNERTagger:\n",
    "    \"\"\"\n",
    "    Extracts entities using StanfordNERTagger\n",
    "    \"\"\"\n",
    "    english_model = './stanford-ner/english.muc.7class.distsim.crf.ser.gz'\n",
    "    jar = './stanford-ner/stanford-ner.jar'\n",
    "\n",
    "    st = StanfordNERTagger(english_model, jar, encoding='utf-8')\n",
    "\n",
    "    def extract_entity_stanford(self, text, entity):\n",
    "        classified_text = self.st.tag(text.split())\n",
    "        #print(' Classified text', classified_text)\n",
    "        extracted_value = []\n",
    "        for item in classified_text:\n",
    "            if item[1] == entity:\n",
    "                extracted_value.append(item[0])\n",
    "        return extracted_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the sentence containing the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "from spacy.symbols import DATE\n",
    "\n",
    "\n",
    "class DateFinder:\n",
    "    \"\"\"\n",
    "    Find the sentence with the event date.\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def stanford_ner_tagger(self, text, entity):\n",
    "        \"\"\"\n",
    "        Extracts entities using Stanford NER Tagger\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s = MyStanfordNERTagger()\n",
    "        tagged_values = s.extract_entity_stanford(text, entity)\n",
    "        return tagged_values\n",
    "\n",
    "    def splitParagraphIntoSentences(self, paragraph):\n",
    "        ''' break a paragraph into sentences\n",
    "            and return a list '''\n",
    "        # to split by multile characters\n",
    "\n",
    "        #   regular expressions are easiest (and fastest)\n",
    "        sentenceEnders = re.compile('[.]')\n",
    "        sentenceList = sentenceEnders.split(paragraph)\n",
    "        return sentenceList\n",
    "\n",
    "    def combine(self, value1, value2):\n",
    "        l = set()\n",
    "        l.add(value1)\n",
    "        l.add(value2)\n",
    "        return l\n",
    "\n",
    "    def get_event_date(self):\n",
    "        \"\"\"\n",
    "        Gets the sentence containing \"a date\" using spaCY NER Tagging.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        f = open('sample_input.csv', 'r')\n",
    "        fieldnames = ['text', 'date_publish', 'event_type_635', 'textrank', 'actor1_635']\n",
    "        dataframe = pd.read_csv(f, names=fieldnames, header=None, skiprows=1)\n",
    "\n",
    "\n",
    "        for index, row in dataframe.iterrows():\n",
    "            try:\n",
    "                # f = open(filename, \"r\").read()\n",
    "                text = row['text']\n",
    "                date_publish = row['date_publish']\n",
    "                label = row['event_type_635']\n",
    "                ordered_entities = row['textrank']\n",
    "                actor1_635 = row['actor1_635']\n",
    "                count = count + 1\n",
    "                # d = json.loads(f)\n",
    "                dict = {}\n",
    "                # for key, value in d.items():\n",
    "                dict[\"date_publish\"] = date_publish\n",
    "                dict[\"label\"] = label\n",
    "                dict[\"actor1_635\"] = actor1_635\n",
    "                dict[\"ordered_entities_from_textrank\"] = ordered_entities\n",
    "                dict[\"full_text\"] = text\n",
    "                sentence_list = self.splitParagraphIntoSentences(text)\n",
    "                event_day = False\n",
    "                dict[\"first_sentence\"] = sentence_list[0]\n",
    "                for i in range(0, len(sentence_list)):\n",
    "                    if event_day is True:\n",
    "                        break\n",
    "                    if event_day is False:\n",
    "                        doc = self.nlp(sentence_list[i])\n",
    "                        # For the same sentence, you check if there is an entity tagged as \"time\". Giving it higher priority.\n",
    "                        # ALSO YOU CAN'T DO THAT, CUZ IT IS NOT ALWAYS CORRECT => \"11-hour long\" gets higher priority than \"Saturday\"\n",
    "                        spacy_time = False\n",
    "                        spacy_date = False\n",
    "                        for ent in doc.ents:\n",
    "                            if spacy_time is False and spacy_date is False:\n",
    "                                if ent.label_ == u'TIME':\n",
    "                                    spacy_time = True\n",
    "                                    spacy_time_value = \"True\"\n",
    "                                    TIME_text = ent.text\n",
    "                                    event_day = True\n",
    "                                    if i != 0:\n",
    "                                        sentence_list[i] = sentence_list[i].replace('\\n', '')\n",
    "                                        sentence_with_date_value = sentence_list[i]\n",
    "                                    else:\n",
    "                                        sentence_with_date_value = u\"Same as first sentence\"\n",
    "                            # doing this because \"time\" tags better entities than date => one-day international today.\n",
    "                                elif ent.label_ == u'DATE':\n",
    "                                    spacy_date = True\n",
    "                                    spacy_date_value = \"True\"\n",
    "                                    DATE_text = ent.text\n",
    "                                    event_day = True\n",
    "                                    if i != 0:\n",
    "                                        sentence_list[i] = sentence_list[i].replace('\\n', '')\n",
    "                                        sentence_with_date_value = sentence_list[i]\n",
    "                                    else:\n",
    "                                        sentence_with_date_value = u\"Same as first sentence\"\n",
    "                            else:\n",
    "                                break\n",
    "                        if event_day is True:\n",
    "                            if spacy_time is True and spacy_date is False:\n",
    "                                dict[\"spaCY_date\"] = spacy_time_value\n",
    "                                dict[\"DATE\"] = TIME_text\n",
    "                                dict[\"sentence_with_date\"] = sentence_with_date_value\n",
    "                            elif spacy_time is False and spacy_date is True:\n",
    "                                dict[\"spaCY_date\"] = spacy_date_value\n",
    "                                dict[\"DATE\"] = DATE_text\n",
    "                                dict[\"sentence_with_date\"] = sentence_with_date_value\n",
    "                            else:\n",
    "                                #giving priority to date here.\n",
    "                                dict[\"spaCY_date\"] = spacy_date_value\n",
    "                                dict[\"DATE\"] = DATE_text\n",
    "                                dict[\"sentence_with_date\"] = sentence_with_date_value\n",
    "                            break\n",
    "                entity = u'DATE'\n",
    "                value = text\n",
    "                stanford_value = self.stanford_ner_tagger(value, entity)\n",
    "                if len(stanford_value) > 0:\n",
    "                    combined_date_values = self.combine(stanford_value[0], DATE_text)\n",
    "                else:\n",
    "                    combined_date_values = \"Null\"\n",
    "                dict[\"stanfordNER_date\"] = stanford_value\n",
    "                dict[\"combined_date\"] = combined_date_values\n",
    "                if event_day is False:\n",
    "                    dict[\"spaCY_date\"] = \"False\"\n",
    "                    dict[\"DATE\"] = \"Null\"\n",
    "                    dict[\"sentence_with_date\"] = \"Null\"\n",
    "                self.l.append(dict)\n",
    "            except Exception as e:\n",
    "                print('Text', text)\n",
    "                print(e)\n",
    "        print(self.l)\n",
    "        with open('./output/list_stored_in_csv.txt', 'w+') as file1:\n",
    "            file1.write(str(self.l))\n",
    "        s = pd.DataFrame(self.l, columns=['date_publish', 'label', 'actor1_635', 'textrank', 'stanfordNER_date', 'combined_date', 'first_sentence', 'spaCY_date', 'DATE', 'sentence_with_date', 'full_text'])\n",
    "        s.to_csv(\"./output/date_and_first_sentence.csv\", encoding=\"utf-8\")\n",
    "        list_to_be_returned = self.l\n",
    "        return list_to_be_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dateparser\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "class ModifyDate():\n",
    "    \"\"\"\n",
    "    Modify date string to a standard format.\n",
    "    \"\"\"\n",
    "    dates_to_be_returned = []\n",
    "    def standardize_date(self, date_publish, date_in_article):\n",
    "        for i, j in zip(date_publish, date_in_article):\n",
    "            try:\n",
    "                i = i.encode(\"ascii\", \"ignore\")\n",
    "                j = j.encode(\"ascii\", \"ignore\")\n",
    "                try:\n",
    "                    datetime_object = datetime.strptime(i, '%Y-%m-%d %H:%M:%S')\n",
    "                    date = dateparser.parse(j, settings={'RELATIVE_BASE': datetime_object})\n",
    "                    if date is not None:\n",
    "                        date = date.strftime('%m/%d/%Y %H:%M:%S')\n",
    "                    else:\n",
    "                        date = None\n",
    "                except Exception as e:\n",
    "                    date = None\n",
    "                if date is None:\n",
    "                    #If there was no proper date mentioned in the article, then copy the published date.\n",
    "                    date = i\n",
    "            except Exception as e:\n",
    "                #if date_publish is None\n",
    "                date = \"2017-01-31 11:59:00\"\n",
    "            self.dates_to_be_returned.append(date)\n",
    "        return self.dates_to_be_returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import gensim\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "class Summarizer:\n",
    "    \"\"\"\n",
    "    Summarizes the given text.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_summary(self, text):\n",
    "        try:\n",
    "            summary = summarize(text, ratio=0.4)\n",
    "            if len(summary.split('.')) <= 2:\n",
    "                print(summary)\n",
    "                print('-'*32)\n",
    "                summary = summarize(text, ratio=0.75)\n",
    "                print(summary)\n",
    "                print('-' * 32)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            summary = text\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract other entities and summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'date_publish': '3/23/15 20:26', 'label': 'Protests', 'actor1_635': 'Protesters (India)', 'ordered_entities_from_textrank': \"['incident', 'Jarada', 'village', 'persons', 'police', 'group', 'groups', 'Odisha']\", 'full_text': 'Odisha Sun Times Bureau Brahmapur, Mar 23 Police today arrested 11 persons in connection with yesterday s group clash in Ambagaon under Jarada police limits in Odisha s Ganjam district. While three persons have sustained injuries in yesterday s group clash, over 40 houses were set ablaze as rival groups set afire thatched houses belonging to the other group and hurled bombs at each other. One seriously injured person has been referred to a hospital at Visakhapatnam while other two are undergoing treatment at Jarada hospital, police said. All male members of the village, which has around 90 households, have fled after the incident fearing arrest. DIG southern range Amitav Thakur who was on a visit to the strife torn village said The situation is now under control. SDPO Chikiti and IIC Jarada are camping in the village to maintain peace. Raids were conducted in different places to nab other persons involved in the incident, said SP Sarthak Sarangi. District administration has provided polythene sheets and food to the families, who had been rendered homeless in the arson, said Prem Chandra Choudhury collector Ganjam. There is old enmity between the groups, but the provocation for the fresh incident is not known and is being investigated, police said.', 'first_sentence': 'Odisha Sun Times Bureau Brahmapur, Mar 23 Police today arrested 11 persons in connection with yesterday s group clash in Ambagaon under Jarada police limits in Odisha s Ganjam district', 'spaCY_date': 'True', 'DATE': 'today', 'sentence_with_date': 'Same as first sentence', 'stanfordNER_date': [], 'combined_date': 'Null'}, {'date_publish': '3/23/19 13:42', 'label': 'Riots', 'actor1_635': 'Rioters (India)', 'ordered_entities_from_textrank': \"['Afiz', 'Ali', 'Police', 'Rizul', 'March', 'death', 'family', 'house']\", 'full_text': 'ABHIJIT TALUKDAR Tamulpur, March 23, 2019 Tension gripped Kamrup district after a man was stabbed to death on March 23rd, 2019. The accused identified as Rizul Ali was at loggerheads with the deceased Afiz Ali over a family dispute. The villagers had called a meeting on March 23rd, 2019 to solve the dispute when the Rizul stabbed Afiz to death owing to disagreements. Afiz was rushed to the hospital but he succumbed to his injuries. Post the death of Afiz, the angry mob vandalized the house of Rizul and also set his car ablaze along with two motorbikes, furniture and other items in the house. The mob also vandalized the house of a family which was trying to protect the accused. As per reports, Kamrup Superintendent of Police along with Rangia Sub Divisional Police Office reached the spot to cool the irate crowd. As per police, Rizul is a dreaded criminal. Locals demanded strict action against the culprit.', 'first_sentence': 'ABHIJIT TALUKDAR Tamulpur, March 23, 2019 Tension gripped Kamrup district after a man was stabbed to death on March 23rd, 2019', 'spaCY_date': 'True', 'DATE': 'March 23,', 'sentence_with_date': 'Same as first sentence', 'stanfordNER_date': ['March', '23,', '2019', 'March', 'March', '23rd,', '2019'], 'combined_date': {'March', 'March 23,'}}, {'date_publish': nan, 'label': 'Protests', 'actor1_635': 'Protesters (India)', 'ordered_entities_from_textrank': \"['M', 'CPI', 'Kamalpur', 'BJP', 'MP', 'Jitendra', 'office', 'Election']\", 'full_text': 'Tripura News 10 CPI M supporters injured after MP Jiten successfully opened BJP occupied Kamalpur CPI M Party office TIWN March 23, 2019 AGARTALA, March 23 TIWN Tension hits Kamalpur after MP Jitendra Chaudhury successfully opened Kamalpur party office of CPI M after 11 months. Centering this office opening multiple attacks came upon CPI M in previous days, which came in media reports. However, today ahead of Lok Sabha Election, this office was opened and a meeting led by MP Jitendra Chaudhury was held, but amid model code of conduct BJP hooligans led massive attack on CPI M. Party source said that amid hooligans threat MP Jitendra Chaudhury went in Kamalpur and opened the office. But, after that BJP goons attacked 10 CPI M supporters in Manikbhander, Kamalpur whereas police remained as mute spectator. SP Dhalai remained as mute spectator of the incident. CPI M said, the administration was functioning as BJP s puppet. At around, 10 am Jitendra Chaudhury reached the spot but BJP goons in front of SDPO held the attack. MP Jitendra Choudhury then contacted SP Dhalai but no development came. CPI M will inform Election Commission about this Police Administration s failure.', 'first_sentence': 'Tripura News 10 CPI M supporters injured after MP Jiten successfully opened BJP occupied Kamalpur CPI M Party office TIWN March 23, 2019 AGARTALA, March 23 TIWN Tension hits Kamalpur after MP Jitendra Chaudhury successfully opened Kamalpur party office of CPI M after 11 months', 'spaCY_date': 'True', 'DATE': '11 months', 'sentence_with_date': 'Same as first sentence', 'stanfordNER_date': ['March', '23,', '2019', 'March', '23'], 'combined_date': {'March', '11 months'}}, {'date_publish': '3/23/19 21:35', 'label': 'Protests', 'actor1_635': 'Protesters (India)', 'ordered_entities_from_textrank': \"['students', 'Gandoh', 'Government', 'school', 'shortage', 'teaching', 'staff', 'Division']\", 'full_text': 'Excelsior Correspondent DODA, Mar 23 The students of Government High School Tilogra in Sub Division Gandoh today staged protest against the shortage of teachers in their school. The students raised slogans against the Education Department and also held Government equally responsible against shortage of teaching staff in their school, wherein students of remote areas also pursue their education. Amid slogans, the protesting students blocked the vehicular movement on the Gandoh Thathri Doda road at Malikpura. They were demanding adequate teaching staff in their school. The protesting students informed that there was no class work in the school due to non availability of the adequate teaching staff. Due to shortage of teaching staff, our studies are suffering , they added. Later, SDM Gandoh Pritam Lal Thapa reached the spot and assured the students of taking up their demand with the Education Department for redressal. On the assurance, the protesting students dispersed.', 'first_sentence': 'Excelsior Correspondent DODA, Mar 23 The students of Government High School Tilogra in Sub Division Gandoh today staged protest against the shortage of teachers in their school', 'spaCY_date': 'True', 'DATE': 'today', 'sentence_with_date': 'Same as first sentence', 'stanfordNER_date': [], 'combined_date': 'Null'}, {'date_publish': '3/23/19 11:30', 'label': 'Protests', 'actor1_635': 'Protesters (India)', 'ordered_entities_from_textrank': \"['police', 'clash', 'groups', 'district', 'Dhepaguda', 'platoons', 'villages', 'area']\", 'full_text': 'Jeypore The Koraput district administration today prohibitory orders under Section 144 of CrPC following a group clash between two villages under Sadar police limits in Koraput district. Sources said four platoons of BSF personnel and three platoons of police force have been deployed in the area to avoid any untoward incident. Besides, district SP KV Singh, IPS Sagarika Nath and four Inspectors In charges IICs have also been engaged in the locality. A flag march will be conducted in the sensitive area. Yesterday, a clash broke out between two groups of Dhepaguda and kundariguda villages over past enmity while they had gone to a nearby canal to take bath after celebrating Holi. Several persons from both the groups were injured due to stone pelting during the clash. When the situation went out of control, police reached the spot and opened blank fires to disperse the mob. Besides, vehicular traffic on NH 26 was disrupted for over two hours due to the clash.', 'first_sentence': 'Jeypore The Koraput district administration today prohibitory orders under Section 144 of CrPC following a group clash between two villages under Sadar police limits in Koraput district', 'spaCY_date': 'True', 'DATE': 'today', 'sentence_with_date': 'Same as first sentence', 'stanfordNER_date': [], 'combined_date': 'Null'}, {'date_publish': '3/26/19 17:46', 'label': 'Protests', 'actor1_635': 'Protesters (India)', 'ordered_entities_from_textrank': \"['Congress', 'Bhubaneswar', 'police', 'Bhawan', 'ticket', 'today', 'Mohammad', 'Rahman']\", 'full_text': 'Bhubaneswar The Commissionerate Police arrested six persons today in connection with vandalism in Congress Bhawan. The police took Sk. Amir Mohammad, Sabir Mohammad, Latifur Rahman, Sajir Khanm Hadaqat Khan, Sayed Tafiq and Biswa Ranjan Khatei into custody today for damaging property at Odisha Congress headquarters in Bhubaneswar on Mar 23. According to police reports, the arrested individuals are neither Congress workers nor ticket aspirants of upcoming polls. Watch video Congress Bhawan vandalised in Odisha capital The miscreants resorted to sabotage on the premises of Congress Bhawan and later locked its gate protesting the decision of the party to allot the ticket for Ekamra Bhubaneswar Assembly seat to Rashmirekha Mohapatra in the impending elections. Following the incident, Odisha Congress filed a complaint in Capital police station. The accused have been forwarded to court, police sources said.', 'first_sentence': 'Bhubaneswar The Commissionerate Police arrested six persons today in connection with vandalism in Congress Bhawan', 'spaCY_date': 'True', 'DATE': 'today', 'sentence_with_date': 'Same as first sentence', 'stanfordNER_date': ['today'], 'combined_date': {'today'}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odisha Sun Times Bureau Brahmapur, Mar 23 Police today arrested 11 persons in connection with yesterday s group clash in Ambagaon under Jarada police limits in Odisha s Ganjam district\n",
      "Odisha Sun Times Bureau Brahmapur, Mar 23 Police today arrested 11 persons in connection with yesterday s group clash in Ambagaon under Jarada police limits in Odisha s Ganjam district\n",
      "ABHIJIT TALUKDAR Tamulpur, March 23, 2019 Tension gripped Kamrup district after a man was stabbed to death on March 23rd, 2019\n",
      "ABHIJIT TALUKDAR Tamulpur, March 23, 2019 Tension gripped Kamrup district after a man was stabbed to death on March 23rd, 2019\n",
      "Tripura News 10 CPI M supporters injured after MP Jiten successfully opened BJP occupied Kamalpur CPI M Party office TIWN March 23, 2019 AGARTALA, March 23 TIWN Tension hits Kamalpur after MP Jitendra Chaudhury successfully opened Kamalpur party office of CPI M after 11 months\n",
      "Tripura News 10 CPI M supporters injured after MP Jiten successfully opened BJP occupied Kamalpur CPI M Party office TIWN March 23, 2019 AGARTALA, March 23 TIWN Tension hits Kamalpur after MP Jitendra Chaudhury successfully opened Kamalpur party office of CPI M after 11 months\n",
      "Excelsior Correspondent DODA, Mar 23 The students of Government High School Tilogra in Sub Division Gandoh today staged protest against the shortage of teachers in their school\n",
      "Excelsior Correspondent DODA, Mar 23 The students of Government High School Tilogra in Sub Division Gandoh today staged protest against the shortage of teachers in their school\n",
      "Jeypore The Koraput district administration today prohibitory orders under Section 144 of CrPC following a group clash between two villages under Sadar police limits in Koraput district\n",
      "Jeypore The Koraput district administration today prohibitory orders under Section 144 of CrPC following a group clash between two villages under Sadar police limits in Koraput district\n",
      "Bhubaneswar The Commissionerate Police arrested six persons today in connection with vandalism in Congress Bhawan\n",
      "Bhubaneswar The Commissionerate Police arrested six persons today in connection with vandalism in Congress Bhawan\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "    \"\"\"\n",
    "    Defines methods to extract entities needed.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def split_paragraph_into_sentences(self, paragraph):\n",
    "        \"\"\" break a paragraph into sentences\n",
    "            and return a list \"\"\"\n",
    "        import re\n",
    "        # to split by multiple characters\n",
    "\n",
    "        # regular expressions are easiest (and fastest)\n",
    "        sentence_enders = re.compile('[.]')\n",
    "        sentence_list = sentence_enders.split(paragraph)\n",
    "        return sentence_list\n",
    "\n",
    "    def get_actor(self):\n",
    "        st = MyStanfordNERTagger()\n",
    "        df = DateFinder()\n",
    "        date_and_main_sentence_list = df.get_event_date()\n",
    "\n",
    "        for dictionary in date_and_main_sentence_list:\n",
    "            # Having each entity as a list just to extract every single mention of its kind from the text.\n",
    "            # Ideally, the first value from this list would be the match. (Inferring from the \"first mention\" rule)\n",
    "            person = []\n",
    "            ORG = []\n",
    "            location = []\n",
    "            region = []\n",
    "\n",
    "            sentence_with_person = []\n",
    "            sentence_with_loc = []\n",
    "            sentence_with_ORG = []\n",
    "            sentence_with_region = []\n",
    "\n",
    "            stanford_person = []\n",
    "            stanford_ORG = []\n",
    "            stanford_location = []\n",
    "\n",
    "            for key, value in dictionary.items():\n",
    "                full_text = dictionary['full_text']\n",
    "                full_text_split = self.split_paragraph_into_sentences(full_text)\n",
    "                if key == 'DATE' and value != 'Null':\n",
    "                    sentence_with_date = dictionary['sentence_with_date']\n",
    "                    first_sentence = False\n",
    "                    if sentence_with_date == u'Same as first sentence':\n",
    "                        sentence_with_date = dictionary['first_sentence']\n",
    "                        first_sentence = True\n",
    "\n",
    "                    window_size = 1\n",
    "                    i = 1\n",
    "                    doc = self.nlp(sentence_with_date)\n",
    "                    current_sentence = sentence_with_date\n",
    "                    before_flag = False\n",
    "                    after_flag = False\n",
    "\n",
    "                    stanford_location = st.extract_entity_stanford(full_text, u'LOCATION')\n",
    "                    stanford_ORG = st.extract_entity_stanford(full_text, u'ORGANIZATION')\n",
    "                    stanford_person = st.extract_entity_stanford(full_text, u'PERSON')\n",
    "\n",
    "                    while i <= window_size + 1:\n",
    "                        # Searching for other entities in the same sentence. (Could be expanded to a window size of 2)\n",
    "                        for ent in doc.ents:\n",
    "                            if ent.label_ == u'PERSON':\n",
    "                                person.append(ent.text)\n",
    "                                sentence_with_person.append(current_sentence)\n",
    "                            elif ent.label_ == u'ORG':\n",
    "                                ORG.append(ent.text)\n",
    "                                sentence_with_ORG.append(current_sentence)\n",
    "                            elif ent.label_ == u'LOC':\n",
    "                                location.append(ent.text)\n",
    "                                sentence_with_loc.append(current_sentence)\n",
    "                            elif ent.label_ == u'GPE':\n",
    "                                region.append(ent.text)\n",
    "                                sentence_with_region.append(current_sentence)\n",
    "\n",
    "                        if first_sentence is True:\n",
    "                            # Search for the entities in the next sentence.\n",
    "                            try:\n",
    "                                print(full_text_split[0])\n",
    "                                next_sentence = full_text_split[i]\n",
    "                                doc = self.nlp(next_sentence)\n",
    "                                i = i + 1\n",
    "                            except Exception as e:\n",
    "                                break\n",
    "                        else:\n",
    "                            if after_flag is False or before_flag is False:\n",
    "                                # Otherwise, search in one sentence previous to the current one. And one in the sentence after the current one.\n",
    "                                try:\n",
    "                                    j = full_text_split.index(sentence_with_date)\n",
    "                                    if before_flag is False:\n",
    "                                        next_sentence = full_text_split[j-1]\n",
    "                                        before_flag = True\n",
    "                                        doc = self.nlp(next_sentence)\n",
    "                                    elif after_flag is False:\n",
    "                                        next_sentence = full_text_split[j+1]\n",
    "                                        after_flag = True\n",
    "                                        doc = self.nlp(next_sentence)\n",
    "                                        # increment only at this block, because i should be unchanged for the first block. i <= window_size + 1\n",
    "                                        i = i + 1\n",
    "                                    current_sentence = next_sentence\n",
    "                                except Exception as e:\n",
    "                                    break\n",
    "                            else:\n",
    "                                break\n",
    "            dictionary.update({'person': person, 'stanford_person': stanford_person, 'ORG': ORG, 'stanford_ORG': stanford_ORG, 'location': location, 'stanford_location': stanford_location, 'region': region, 'sentence_with_person': sentence_with_person,\n",
    "                               'sentence_with_ORG': sentence_with_ORG, 'sentence_with_loc': sentence_with_loc, 'sentence_with_region': sentence_with_region})\n",
    "            summarizer = Summarizer()\n",
    "            summarized_text = summarizer.get_summary(full_text)\n",
    "            dictionary.update({'summary': summarized_text})\n",
    "\n",
    "        for d in date_and_main_sentence_list:\n",
    "            summary = d.get(\"summary\")\n",
    "            sdate = d.get(\"sentence_with_date\")\n",
    "            sregion = d.get(\"sentence_with_region\")\n",
    "            sperson = d.get(\"sentence_with_person\")\n",
    "            sORG = d.get(\"sentence_with_ORG\")\n",
    "            sloc = d.get(\"sentence_with_loc\")\n",
    "            summary_split = self.split_paragraph_into_sentences(summary)\n",
    "\n",
    "            try:\n",
    "                # First mentioned rule. So we are checking if the sentence of the first entity has been mentioned.\n",
    "                if sdate == u'Same as first sentence':\n",
    "                    text = d.get(\"full_text\")\n",
    "                    text_list = self.split_paragraph_into_sentences(text)\n",
    "                    sdate = text_list[0]\n",
    "                a = summary_split.index(sdate)\n",
    "                d.update({'date_check': 'True'})\n",
    "            except ValueError as exp:\n",
    "                d.update({'date_check': 'False'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                b = summary_split.index(sregion[0])\n",
    "                d.update({'region_check': 'True'})\n",
    "            except ValueError as exp:\n",
    "                d.update({'region_check': 'False'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                c = summary_split.index(sperson[0])\n",
    "                d.update({'person_check': 'True'})\n",
    "            except ValueError as exp:\n",
    "                d.update({'person_check': 'False'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                d2 = summary_split.index(sORG[0])\n",
    "                d.update({'ORG_check': 'True'})\n",
    "            except ValueError as exp:\n",
    "                d.update({'ORG_check': 'False'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                e = summary_split.index(sloc[0])\n",
    "                d.update({'Loc_check': 'True'})\n",
    "            except ValueError as exp:\n",
    "                d.update({'Loc_check': 'False'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        s = pd.DataFrame(date_and_main_sentence_list, columns=['date_publish', 'label', 'actor1_635', 'textrank', 'stanfordNER_date', 'combined_date', 'first_sentence', 'spaCY_date', 'DATE', 'sentence_with_date', 'full_text', 'person', 'stanford_person', 'ORG', 'stanford_ORG', 'location', 'stanford_location', 'region',\n",
    "                                                               'sentence_with_person', 'sentence_with_ORG', 'sentence_with_loc', 'sentence_with_region', 'summary',\n",
    "                                                               'date_check', 'region_check', 'person_check', 'ORG_check', 'Loc_check'])\n",
    "        m = ModifyDate()\n",
    "        date_publish_column = s['date_publish'].tolist()\n",
    "        date_in_article = s['DATE'].tolist()\n",
    "        standardized_dates = m.standardize_date(date_publish_column, date_in_article)\n",
    "\n",
    "        s['standardized_dates'] = standardized_dates\n",
    "\n",
    "        s.to_csv(\"./output/extracted_data.csv\", encoding=\"utf-8\")\n",
    "        summary_list = []\n",
    "        with open('./summary.txt', 'w+') as summary_file:\n",
    "            for d in date_and_main_sentence_list:\n",
    "                compare_summary = dict()\n",
    "                compare_summary[\"text\"] = d.get(\"full_text\")\n",
    "                compare_summary[\"summary\"] = d.get(\"summary\")\n",
    "                summary_list.append(compare_summary)\n",
    "            summary_file.write(str(summary_list))\n",
    "\n",
    "\n",
    "e = Extractor()\n",
    "e.get_actor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the slots by scoring each tagged value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   date_publish     label          actor1_635  \\\n",
      "0           0  3/23/15 20:26  Protests  Protesters (India)   \n",
      "1           1  3/23/19 13:42     Riots     Rioters (India)   \n",
      "2           2            NaN  Protests  Protesters (India)   \n",
      "3           3  3/23/19 21:35  Protests  Protesters (India)   \n",
      "4           4  3/23/19 11:30  Protests  Protesters (India)   \n",
      "5           5  3/26/19 17:46  Protests  Protesters (India)   \n",
      "\n",
      "                                            textrank  \\\n",
      "0  ['incident', 'Jarada', 'village', 'persons', '...   \n",
      "1  ['Afiz', 'Ali', 'Police', 'Rizul', 'March', 'd...   \n",
      "2  ['M', 'CPI', 'Kamalpur', 'BJP', 'MP', 'Jitendr...   \n",
      "3  ['students', 'Gandoh', 'Government', 'school',...   \n",
      "4  ['police', 'clash', 'groups', 'district', 'Dhe...   \n",
      "5  ['Congress', 'Bhubaneswar', 'police', 'Bhawan'...   \n",
      "\n",
      "                                    stanfordNER_date           combined_date  \\\n",
      "0                                                 []                    Null   \n",
      "1  ['March', '23,', '2019', 'March', 'March', '23...  {'March', 'March 23,'}   \n",
      "2            ['March', '23,', '2019', 'March', '23']  {'March', '11 months'}   \n",
      "3                                                 []                    Null   \n",
      "4                                                 []                    Null   \n",
      "5                                          ['today']               {'today'}   \n",
      "\n",
      "                                      first_sentence  spaCY_date       DATE  \\\n",
      "0  Odisha Sun Times Bureau Brahmapur, Mar 23 Poli...        True      today   \n",
      "1  ABHIJIT TALUKDAR Tamulpur, March 23, 2019 Tens...        True  March 23,   \n",
      "2  Tripura News 10 CPI M supporters injured after...        True  11 months   \n",
      "3  Excelsior Correspondent DODA, Mar 23 The stude...        True      today   \n",
      "4  Jeypore The Koraput district administration to...        True      today   \n",
      "5  Bhubaneswar The Commissionerate Police arreste...        True      today   \n",
      "\n",
      "                         ...                          \\\n",
      "0                        ...                           \n",
      "1                        ...                           \n",
      "2                        ...                           \n",
      "3                        ...                           \n",
      "4                        ...                           \n",
      "5                        ...                           \n",
      "\n",
      "                                             summary date_check region_check  \\\n",
      "0  Odisha Sun Times Bureau Brahmapur, Mar 23 Poli...       True         True   \n",
      "1  The accused identified as Rizul Ali was at log...      False          NaN   \n",
      "2  However, today ahead of Lok Sabha Election, th...      False        False   \n",
      "3  Excelsior Correspondent DODA, Mar 23 The stude...       True          NaN   \n",
      "4  Jeypore The Koraput district administration to...       True         True   \n",
      "5  According to police reports, the arrested indi...      False          NaN   \n",
      "\n",
      "  person_check ORG_check Loc_check standardized_dates  \\\n",
      "0          NaN      True       NaN   b'3/23/15 20:26'   \n",
      "1        False     False       NaN   b'3/23/19 13:42'   \n",
      "2        False     False       NaN      1/31/17 11:59   \n",
      "3          NaN      True       NaN   b'3/23/19 21:35'   \n",
      "4         True      True      True   b'3/23/19 11:30'   \n",
      "5        False     False       NaN   b'3/26/19 17:46'   \n",
      "\n",
      "                                    intersect-person  \\\n",
      "0                                                 []   \n",
      "1      [{'Afiz': 1.0}, {'Ali': 1.0}, {'Rizul': 1.0}]   \n",
      "2             [{'Kamalpur': 1.0}, {'Jitendra': 1.0}]   \n",
      "3                                                 []   \n",
      "4                                                 []   \n",
      "5  [{'Bhawan': 1.0}, {'Mohammad,': 0.941176470588...   \n",
      "\n",
      "                                       intersect-ORG  \\\n",
      "0  [{'Jarada': 1.0}, {'Police': 0.8333333333333334}]   \n",
      "1    [{'Ali': 1.0}, {'Police': 1.0}, {'Rizul': 1.0}]   \n",
      "2  [{'M.': 0.6666666666666666}, {'CPI': 1.0}, {'K...   \n",
      "3  [{'Gandoh': 1.0}, {'Government': 1.0}, {'Schoo...   \n",
      "4                                                 []   \n",
      "5  [{'Congress': 1.0}, {'Bhubaneswar': 1.0}, {'Po...   \n",
      "\n",
      "                                  intersect-location  \n",
      "0                    [Jarada, Odisha, Visakhapatnam]  \n",
      "1                                                 []  \n",
      "2                   [Kamalpur, Kamalpur, Lok, Sabha]  \n",
      "3                            [Gandoh, Thathri, Doda]  \n",
      "4  [Sadar, Koraput, Koraput, district., Sources, ...  \n",
      "5                              [Bhubaneswar, Odisha]  \n",
      "\n",
      "[6 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import difflib\n",
    "from builtins import any as b_any\n",
    "\n",
    "def get_filled_slots():\n",
    "    csvfile = open('./output/extracted_data.csv', 'r')\n",
    "   \n",
    "    csv_reader1 = csv.reader(csvfile)\n",
    "    \n",
    "    df1 = pd.read_csv(csvfile)\n",
    "    \n",
    "    person = []\n",
    "    ORG = []\n",
    "    text_rank = []\n",
    "    region = []\n",
    "    event_type_635 = []\n",
    "    full_text = []\n",
    "\n",
    "    for index, row in df1.iterrows():\n",
    "        person_combined = []\n",
    "        ORG_combined = []\n",
    "        location_combined = []\n",
    "\n",
    "        textrank = ast.literal_eval(row['textrank'])\n",
    "        spacy_person = ast.literal_eval(row['person'])\n",
    "        stanford_person = ast.literal_eval(row['stanford_person'])\n",
    "\n",
    "        spacy_ORG = ast.literal_eval(row['ORG'])\n",
    "        stanford_ORG = ast.literal_eval(row['stanford_ORG'])\n",
    "\n",
    "        spacy_location = ast.literal_eval(row['location'])\n",
    "        spacy_region = ast.literal_eval(row['region'])\n",
    "        stanford_location = ast.literal_eval(row['stanford_location'])\n",
    "\n",
    "        for item in spacy_person:\n",
    "            person_combined.append(item)\n",
    "        for item in stanford_person:\n",
    "            person_combined.append(item)\n",
    "\n",
    "        for item in spacy_ORG:\n",
    "            ORG_combined.append(item)\n",
    "        for item in stanford_ORG:\n",
    "            ORG_combined.append(item)\n",
    "        for item in spacy_location:\n",
    "            location_combined.append(item)\n",
    "        for item in spacy_region:\n",
    "            location_combined.append(item)\n",
    "        for item in stanford_location:\n",
    "            location_combined.append(item)\n",
    "\n",
    "        event_type_635.append(str(row['actor1_635']))\n",
    "        full_text.append(str(row['full_text']))\n",
    "        person.append(person_combined)\n",
    "        ORG.append(ORG_combined)\n",
    "        text_rank.append(textrank)\n",
    "        region.append(location_combined)\n",
    "\n",
    "    intersect_person = []\n",
    "    intersect_ORG = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    return_value = True\n",
    "\n",
    "    for item in text_rank:\n",
    "        possible_person = []\n",
    "        possible_ORG = []\n",
    "        choices_person = person[count]\n",
    "        choices_ORG = ORG[count]\n",
    "        for v in item:\n",
    "            score = difflib.get_close_matches(v, choices_person, n=1, cutoff=0.6)\n",
    "            #sort\n",
    "            if score:\n",
    "                d = {}\n",
    "                ratio = difflib.SequenceMatcher(None, v, score[0]).ratio()\n",
    "                d[score[0]] = ratio\n",
    "                possible_person.append(d)\n",
    "            score2 = difflib.get_close_matches(v, choices_ORG, n=1, cutoff=0.6)\n",
    "            if score2:\n",
    "                d = {}\n",
    "                ratio = difflib.SequenceMatcher(None, v, score2[0]).ratio()\n",
    "                d[score2[0]] = ratio\n",
    "                possible_ORG.append(d)\n",
    "        intersect_person.append(possible_person)\n",
    "        intersect_ORG.append(possible_ORG)\n",
    "        count = count + 1\n",
    "\n",
    "    # Create a new csv with the additional columns containing the scored values for each slot.\n",
    "    if return_value is True:\n",
    "        df1['intersect-person'] = intersect_person\n",
    "        df1['intersect-ORG'] = intersect_ORG\n",
    "        df1['intersect-location'] = region\n",
    "        df1.to_csv('./output/scored_entities.csv', encoding='utf-8')\n",
    "        print(df1)\n",
    "        \n",
    "\n",
    "get_filled_slots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the latitude and the longitude for the locations - For UI purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visakhapatnam,Andhra Pradesh,India\n",
      "Odisha,Odisha,India\n",
      "Jarada,Florida,United States\n",
      "Sabha,Sabha District,Libya\n",
      "Lok,Oklahoma,United States\n",
      "Kamalpur,Tripura,India\n",
      "can only concatenate str (not \"NoneType\") to str\n",
      "Doda,Minnesota,United States\n",
      "can only concatenate str (not \"NoneType\") to str\n",
      "Sources,British Columbia,Canada\n",
      "Dhepaguda,Odisha,India\n",
      "Koraput,Odisha,India\n",
      "Sadar,Pennsylvania,United States\n",
      "district.,Kansas,United States\n",
      "Odisha,Odisha,India\n",
      "Bhubaneswar,Odisha,India\n",
      "[['Visakhapatnam,Andhra Pradesh,India', 'Odisha,Odisha,India', 'Jarada,Florida,United States'], [], ['Sabha,Sabha District,Libya', 'Lok,Oklahoma,United States', 'Kamalpur,Tripura,India'], ['Gandoh', 'Doda,Minnesota,United States', 'Thathri'], ['Sources,British Columbia,Canada', 'Dhepaguda,Odisha,India', 'Koraput,Odisha,India', 'Sadar,Pennsylvania,United States', 'district.,Kansas,United States'], ['Odisha,Odisha,India', 'Bhubaneswar,Odisha,India']]\n",
      "[{'Visakhapatnam,Andhra Pradesh,India': (17.7231276, 83.3012842)}, {'Odisha,Odisha,India': (20.9206382, 85.2222895)}, {'Jarada,Florida,United States': (30.480097, -87.295201)}]\n",
      "[]\n",
      "[{'Sabha,Sabha District,Libya': (27.0331541, 14.4316929)}, {'Lok,Oklahoma,United States': (40.5814403, -79.5740244)}, {'Kamalpur,Tripura,India': (24.1687634, 91.8124385)}]\n",
      "[{'Thathri': (32.2360686, 76.3655013)}]\n",
      "[{'Koraput,Odisha,India': (19.0, 83.0)}]\n",
      "[{'Odisha,Odisha,India': (20.9206382, 85.2222895)}, {'Bhubaneswar,Odisha,India': (20.2665668, 85.8437586)}]\n",
      "[[{'Visakhapatnam,Andhra Pradesh,India': (17.7231276, 83.3012842)}, {'Odisha,Odisha,India': (20.9206382, 85.2222895)}, {'Jarada,Florida,United States': (30.480097, -87.295201)}], [], [{'Sabha,Sabha District,Libya': (27.0331541, 14.4316929)}, {'Lok,Oklahoma,United States': (40.5814403, -79.5740244)}, {'Kamalpur,Tripura,India': (24.1687634, 91.8124385)}], [{'Thathri': (32.2360686, 76.3655013)}], [{'Koraput,Odisha,India': (19.0, 83.0)}], [{'Odisha,Odisha,India': (20.9206382, 85.2222895)}, {'Bhubaneswar,Odisha,India': (20.2665668, 85.8437586)}]]\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "import ast\n",
    "import time\n",
    "import geocoder\n",
    "import json\n",
    "\n",
    "\n",
    "def find_geo_code(intersect_location):\n",
    "    # geolocator = Nominatim(user_agent=\"NER_tagger\")\n",
    "\n",
    "    location_for_all_rows_with_geo_code = []\n",
    "    f1 = open('./output/dummy_file1.json', 'a+')\n",
    "    geolocator = Nominatim(user_agent=\"NER_Tagger\")\n",
    "    for location in intersect_location:\n",
    "        geo_codes_per_row = []\n",
    "        for each_location in location:\n",
    "            try:\n",
    "                loc = geolocator.geocode(each_location)\n",
    "                if loc is not None:\n",
    "                    lat_long = (loc.latitude, loc.longitude)\n",
    "                    dic = {}\n",
    "                    dic[each_location] = lat_long\n",
    "                    geo_codes_per_row.append(dic)\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                time.sleep(2)\n",
    "                print(e)\n",
    "                pass\n",
    "        print(geo_codes_per_row)\n",
    "        location_for_all_rows_with_geo_code.append(geo_codes_per_row)\n",
    "        f1.write(json.dumps(location_for_all_rows_with_geo_code))\n",
    "    print(location_for_all_rows_with_geo_code)\n",
    "    return location_for_all_rows_with_geo_code\n",
    "\n",
    "\n",
    "def find_country(intersect_location):\n",
    "    intersect_location_updated = []\n",
    "    f2 = open('./output/dummy_file2.json', 'a+')\n",
    "    for location in intersect_location:\n",
    "        list_of_updated_locations = []\n",
    "        for each_location in location:\n",
    "            try:\n",
    "                g = geocoder.google(each_location, key='YOUR_API_KEY')\n",
    "                country = g.country_long\n",
    "                state = g.state_long\n",
    "                updated_location = each_location + ',' + state + ',' + country\n",
    "                print(updated_location)\n",
    "                list_of_updated_locations.append(updated_location)\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(2)\n",
    "                list_of_updated_locations.append(each_location)\n",
    "                pass\n",
    "        f2.write(json.dumps(intersect_location_updated))\n",
    "        intersect_location_updated.append(list_of_updated_locations)\n",
    "    print(intersect_location_updated)\n",
    "    return intersect_location_updated\n",
    "\n",
    "\n",
    "csvfile = open('./output/scored_entities.csv', 'r')\n",
    "df1 = pd.read_csv(csvfile)\n",
    "\n",
    "intersect_location = []\n",
    "for index, row in df1.iterrows():\n",
    "    loc = ast.literal_eval(row['intersect-location'])\n",
    "    intersect_location.append(loc)\n",
    "\n",
    "intersect_location1 = intersect_location\n",
    "location_set = []\n",
    "\n",
    "for v in intersect_location1:\n",
    "    l = list(set(v))\n",
    "    location_set.append(l)\n",
    "\n",
    "intersect_location_updated = find_country(intersect_location=location_set)\n",
    "\n",
    "location_in_csv = find_geo_code(intersect_location_updated)\n",
    "\n",
    "lat_long_column = []\n",
    "for each_location in location_in_csv:\n",
    "    lat_long_per_row = []\n",
    "    for each_dict in each_location:\n",
    "        for key, value in each_dict.items():\n",
    "            lat_long_per_row.append(value)\n",
    "    lat_long_column.append(lat_long_per_row)\n",
    "\n",
    "df1['lat-long-with-cities'] = location_in_csv\n",
    "df1['lat-long'] = lat_long_column\n",
    "\n",
    "df1.to_csv('./output/final_data_with_lat_and_long.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
